{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from google.colab import drive\n",
    "from tokenizers import Tokenizer\n",
    "from abc import ABC, abstractmethod\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "WORKING_DIR = \"/content/drive/My Drive/Text-Generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(3000)\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    torch.cuda.manual_seed_all(3000)\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "random.seed(3000)\n",
    "\n",
    "VOCAB_SIZE=25000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "INIT_LR = 2e-04\n",
    "SEQ_LEN = 52\n",
    "D_MODEL = 512\n",
    "N_BLOCKS = 6\n",
    "HEADS = 16\n",
    "DROPOUT = 0.1\n",
    "DFF = 2048\n",
    "MODELS_FOLDER = f\"{WORKING_DIR}/models\"\n",
    "PRELOAD_MODEL_FILEPATH = \"\"\n",
    "TOKENIZER_FILEPATH = f\"{WORKING_DIR}/tokenizers/amharic-bpe-tokenizer-v1-{VOCAB_SIZE // 1000}k.json\"\n",
    "TB_LOG_DIR = \"logs/gpt_model\"\n",
    "DATASET_PATH = f\"{WORKING_DIR}/data/amharic-texts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingPipeline(ABC):   \n",
    "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    @abstractmethod\n",
    "    def preprocess(self, text: str, encode=True) -> str:\n",
    "        pass\n",
    "\n",
    "    \n",
    "class AmharicPreprocessor(PreprocessingPipeline):\n",
    "    def __init__(self, tokenizer: Tokenizer) -> None:\n",
    "        super().__init__(tokenizer)\n",
    "    \n",
    "    def preprocess(self, text: str, encode=True) -> str:\n",
    "        # Character level mismatch\n",
    "        text = self.normalize_char_level_missmatch(text)\n",
    "        \n",
    "        # Replace commonly used abbreviations\n",
    "        text = self.normalize_abbreviations(text)\n",
    "        \n",
    "        if encode:\n",
    "            return self.tokenizer.encode(\n",
    "                text,\n",
    "            ).ids\n",
    "        else:\n",
    "            return text\n",
    "            \n",
    "    # Remove abbreviations\n",
    "    def normalize_abbreviations(self, text: str) -> str:\n",
    "        common_amharic_abbreviations = {\n",
    "            \"ት/ቤት\": \"ትምህርት ቤት\",\n",
    "            \"ት/ርት\": \"ትምህርት\",\n",
    "            \"ት/ክፍል\": \"ትምህርት ክፍል\",\n",
    "            \"ሃ/አለቃ\": \"ሃምሳ አለቃ\",\n",
    "            \"ሃ/ስላሴ\": \"ሃይለ ስላሴ\",\n",
    "            \"ደ/ዘይት\": \"ደብረ ዘይት\",\n",
    "            \"ደ/ታቦር\": \"ደብረ ታቦር\",\n",
    "            \"መ/ር\": \"መምህር\",\n",
    "            \"መ/ቤት\": \"መስሪያ ቤት\",\n",
    "            \"መ/አለቃ\": \"መቶ አለቃ\",\n",
    "            \"ክ/ከተማ\": \"ክፍለ ከተማ\",\n",
    "            \"ክ/ሀገር\": \"ክፍለ ሀገር\",\n",
    "            \"ወ/ር\": \"\",\n",
    "            \"ወ/ሮ\": \"ወይዘሮ\",\n",
    "            \"ወ/ሪት\": \"ወይዘሪት\",\n",
    "            \"ወ/ስላሴ\": \"ወልደ ስላሴ\",\n",
    "            \"ፍ/ስላሴ\": \"ፍቅረ ስላሴ\",\n",
    "            \"ፍ/ቤት\": \"ፍርድ ቤት\",\n",
    "            \"ጽ/ቤት\": \"ጽህፈት ቤት\",\n",
    "            \"ሲ/ር\": \"\",\n",
    "            \"ፕ/ር\": \"ፕሮፌሰር\",\n",
    "            \"ጠ/ሚንስትር\": \"ጠቅላይ ሚኒስተር\",\n",
    "            \"ጠ/ሚ\": \"ጠቅላይ ሚኒስተር\",\n",
    "            \"ዶ/ር\": \"ዶክተር\",\n",
    "            \"ገ/ገዮርጊስ\": \"ገብረ ገዮርጊስ\",\n",
    "            \"ቤ/ክርስትያን\": \"ቤተ ክርስትያን\",\n",
    "            \"ም/ስራ\": \"\",\n",
    "            \"ም/ቤት\": \"ምክር ቤተ\",\n",
    "            \"ተ/ሃይማኖት\": \"ተክለ ሃይማኖት\",\n",
    "            \"ሚ/ር\": \"ሚኒስትር\",\n",
    "            \"ኮ/ል\": \"ኮሎኔል\",\n",
    "            \"ሜ/ጀነራል\": \"ሜጀር ጀነራል\",\n",
    "            \"ብ/ጀነራል\": \"ብርጋደር ጀነራል\",\n",
    "            \"ሌ/ኮለኔል\": \"ሌተናንት ኮለኔል\",\n",
    "            \"ሊ/መንበር\": \"ሊቀ መንበር\",\n",
    "            \"አ/አ\": \"ኣዲስ ኣበባ\",\n",
    "            \"አ.አ\": \"ኣዲስ ኣበባ\",\n",
    "            \"ር/መምህር\": \"ርዕሰ መምህር\",\n",
    "            \"ፕ/ት\": \"\",\n",
    "            \"ዓም\": \"ዓመተ ምህረት\",\n",
    "            \"ዓ.ዓ\": \"ዓመተ ዓለም\",\n",
    "        }\n",
    "        for key in common_amharic_abbreviations:\n",
    "            regex = rf'\\b{re.escape(key)}\\b'\n",
    "            text = re.sub(regex, common_amharic_abbreviations[key], text)\n",
    "\n",
    "        # Remove punctuation, numbers, and extra spaces\n",
    "        text = re.sub(r'[.\\?\"\\',/#!$%^&*;:፤።{}=\\-_`~()፩፪፫፬፭፮፮፰፱፲፳፴፵፵፷፸፹፺፻01-9]', ' ', text)\n",
    "        text = re.sub(r'\\s{2,}', ' ', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    #method to normalize character level missmatch such as ጸሀይ and ፀሐይ\n",
    "    def normalize_char_level_missmatch(self, text: str) -> str:\n",
    "        rep1=re.sub('[ሃኅኃሐሓኻ]','ሀ',text)\n",
    "        rep2=re.sub('[ሑኁዅ]','ሁ',rep1)\n",
    "        rep3=re.sub('[ኂሒኺ]','ሂ',rep2)\n",
    "        rep4=re.sub('[ኌሔዄ]','ሄ',rep3)\n",
    "        rep5=re.sub('[ሕኅ]','ህ',rep4)\n",
    "        rep6=re.sub('[ኆሖኾ]','ሆ',rep5)\n",
    "        rep7=re.sub('[ሠ]','ሰ',rep6)\n",
    "        rep8=re.sub('[ሡ]','ሱ',rep7)\n",
    "        rep9=re.sub('[ሢ]','ሲ',rep8)\n",
    "        rep10=re.sub('[ሣ]','ሳ',rep9)\n",
    "        rep11=re.sub('[ሤ]','ሴ',rep10)\n",
    "        rep12=re.sub('[ሥ]','ስ',rep11)\n",
    "        rep13=re.sub('[ሦ]','ሶ',rep12)\n",
    "        rep14=re.sub('[ዓኣዐ]','አ',rep13)\n",
    "        rep15=re.sub('[ዑ]','ኡ',rep14)\n",
    "        rep16=re.sub('[ዒ]','ኢ',rep15)\n",
    "        rep17=re.sub('[ዔ]','ኤ',rep16)\n",
    "        rep18=re.sub('[ዕ]','እ',rep17)\n",
    "        rep19=re.sub('[ዖ]','ኦ',rep18)\n",
    "        rep20=re.sub('[ጸ]','ፀ',rep19)\n",
    "        rep21=re.sub('[ጹ]','ፁ',rep20)\n",
    "        rep22=re.sub('[ጺ]','ፂ',rep21)\n",
    "        rep23=re.sub('[ጻ]','ፃ',rep22)\n",
    "        rep24=re.sub('[ጼ]','ፄ',rep23)\n",
    "        rep25=re.sub('[ጽ]','ፅ',rep24)\n",
    "        rep26=re.sub('[ጾ]','ፆ',rep25)\n",
    "        #Normalizing words with Labialized Amharic characters such as በልቱዋል or  በልቱአል to  በልቷል  \n",
    "        rep27=re.sub('(ሉ[ዋአ])','ሏ',rep26)\n",
    "        rep28=re.sub('(ሙ[ዋአ])','ሟ',rep27)\n",
    "        rep29=re.sub('(ቱ[ዋአ])','ቷ',rep28)\n",
    "        rep30=re.sub('(ሩ[ዋአ])','ሯ',rep29)\n",
    "        rep31=re.sub('(ሱ[ዋአ])','ሷ',rep30)\n",
    "        rep32=re.sub('(ሹ[ዋአ])','ሿ',rep31)\n",
    "        rep33=re.sub('(ቁ[ዋአ])','ቋ',rep32)\n",
    "        rep34=re.sub('(ቡ[ዋአ])','ቧ',rep33)\n",
    "        rep35=re.sub('(ቹ[ዋአ])','ቿ',rep34)\n",
    "        rep36=re.sub('(ሁ[ዋአ])','ኋ',rep35)\n",
    "        rep37=re.sub('(ኑ[ዋአ])','ኗ',rep36)\n",
    "        rep38=re.sub('(ኙ[ዋአ])','ኟ',rep37)\n",
    "        rep39=re.sub('(ኩ[ዋአ])','ኳ',rep38)\n",
    "        rep40=re.sub('(ዙ[ዋአ])','ዟ',rep39)\n",
    "        rep41=re.sub('(ጉ[ዋአ])','ጓ',rep40)\n",
    "        rep42=re.sub('(ደ[ዋአ])','ዷ',rep41)\n",
    "        rep43=re.sub('(ጡ[ዋአ])','ጧ',rep42)\n",
    "        rep44=re.sub('(ጩ[ዋአ])','ጯ',rep43)\n",
    "        rep45=re.sub('(ጹ[ዋአ])','ጿ',rep44)\n",
    "        rep46=re.sub('(ፉ[ዋአ])','ፏ',rep45)\n",
    "        rep47=re.sub('[ቊ]','ቁ',rep46) #ቁ can be written as ቊ\n",
    "        rep48=re.sub('[ኵ]','ኩ',rep47) #ኩ can be also written as ኵ  \n",
    "        \n",
    "        return rep48\n",
    "\n",
    "    #replacing any existance of special character or punctuation to null  \n",
    "    def remove_punc_and_special_chars(self, text: str) -> str: # puct in amh =፡።፤;፦፧፨፠፣ \n",
    "        normalized_text = re.sub('[\\!\\@\\#\\$\\%\\^\\«\\»\\&\\*\\(\\)\\…\\[\\]\\{\\}\\;\\“\\”\\›\\’\\‘\\\"\\'\\:\\,\\.\\‹\\/\\<\\>\\?\\\\\\\\|\\`\\´\\~\\-\\=\\+\\፡\\።\\፤\\;\\፦\\፥\\፧\\፨\\፠\\፣]', '', text) \n",
    "        return normalized_text\n",
    "\n",
    "    #remove all ascii characters and Arabic and Amharic numbers\n",
    "    def remove_ascii_and_numbers(self, text: str) -> str:\n",
    "        rm_num_and_ascii=re.sub('[A-Za-z0-9]','',text)\n",
    "        return re.sub('[^\\u1200-\\u137F\\s]+','',rm_num_and_ascii)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts: list[str], tokenizer: Tokenizer) -> None:\n",
    "        super().__init__()\n",
    "        self.texts: list[str] = texts\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.preprocessor = AmharicPreprocessor(tokenizer)\n",
    "        \n",
    "        # (1,)\n",
    "        self.pad_token = torch.tensor([self.tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def batch_iterator(self, batch_size: int) -> DataLoader:\n",
    "        return DataLoader(self, batch_size, shuffle=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def lookback_mask(size: int) -> torch.Tensor:\n",
    "        # Lower triangular matrix\n",
    "        # [[\n",
    "        #   [1, 0, ... , 0],\n",
    "        #   [1, 1, ... , 0],\n",
    "        #   [1, 1, ... , 0],\n",
    "        #   [1, 1, ... , 1]\n",
    "        # ]] \n",
    "        # 1 x size x size\n",
    "        return torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int) == 0\n",
    "    \n",
    "    def shift_left(self, list: list[str]) -> list[str]:\n",
    "        return [list[i] for i in range(1, len(list))] + [self.tokenizer.token_to_id(\"[UNK]\")]\n",
    "    \n",
    "    def __getitem__(self, index) -> dict:\n",
    "        token_ids = self.preprocessor.preprocess(self.texts[index])\n",
    "        padding = SEQ_LEN - len(token_ids)\n",
    "       \n",
    "        # (seq_len,)\n",
    "        decoder_input = torch.concat([\n",
    "            # (len(token_ids),)\n",
    "            torch.tensor(token_ids, dtype=torch.int64),\n",
    "\n",
    "            # (padding,)\n",
    "            torch.tensor([self.pad_token] * padding, dtype=torch.int64)\n",
    "        ])                    \n",
    "        \n",
    "        # (seq_len,)\n",
    "        label = torch.concat([\n",
    "            # (len(token_ids),)\n",
    "            torch.tensor(self.shift_left(token_ids), dtype=torch.int64),\n",
    "\n",
    "            # (padding,)\n",
    "            torch.tensor([self.pad_token] * padding, dtype=torch.int64)\n",
    "        ])     \n",
    "        \n",
    "        return {\n",
    "            # (seq_len,)\n",
    "            \"decoder_input\": decoder_input,\n",
    "                            \n",
    "            # (seq_len,) != (1,) --> (seq_len,) --> (1, 1, seq_len) --> (1, seq_len) & (1, seq_len, seq_len) --> (1, seq_len, seq_len)\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & self.lookback_mask(SEQ_LEN),  \n",
    "\n",
    "            # (seq_len,)         \n",
    "            \"label\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding: nn.Embedding = nn.Embedding(vocab_size, D_MODEL)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (batch, seq_len, 1) --> (batch, seq_len, d_model)\n",
    "        return self.embedding.forward(x) * math.sqrt(D_MODEL)\n",
    "    \n",
    "    \n",
    "class PositionEncoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()        \n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        \n",
    "        # (seq_len, d_model)\n",
    "        pe = torch.zeros(SEQ_LEN, D_MODEL)\n",
    "        \n",
    "        # (seq_len, 1)\n",
    "        pos = torch.arange(0, SEQ_LEN, dtype=torch.float).float().unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, D_MODEL, 2).float() * -(math.log(10000.0) / D_MODEL))\n",
    "\n",
    "        # PE(pos, 2i) = sin(pos / (10000 ^ (2i/d_model)))\n",
    "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "\n",
    "        # PE(pos, 2i + 1) = cos(pos / (10000 ^ (2i/d_model)))\n",
    "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "        \n",
    "        # (1, seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0) \n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        assert x.shape[1] <= SEQ_LEN, f\"Input sequence length exceeds the position encoder's max sequence length  `{SEQ_LEN}`\"\n",
    "\n",
    "        # (batch, seq_len, d_model) + (1, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        return self.dropout(x + self.pe[:, :x.shape[1], :].requires_grad_(False))\n",
    "    \n",
    "    \n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(D_MODEL, DFF).to(DEVICE)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.linear_2 = nn.Linear(DFF, D_MODEL).to(DEVICE)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, dff) -> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "    \n",
    "    \n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        assert D_MODEL % HEADS == 0, \"d_model is not divisible by heads\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_k = D_MODEL // HEADS\n",
    "        \n",
    "        self.W_q = nn.Linear(D_MODEL, D_MODEL, bias=False).to(DEVICE)\n",
    "        self.W_k = nn.Linear(D_MODEL, D_MODEL, bias=False).to(DEVICE)\n",
    "        self.W_v = nn.Linear(D_MODEL, D_MODEL, bias=False).to(DEVICE)\n",
    "\n",
    "        self.W_o = nn.Linear(D_MODEL, D_MODEL, bias=False).to(DEVICE)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(\n",
    "        # (batch, head, seq_len, d_k)\n",
    "        query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, \n",
    "        dropout: nn.Dropout=None, \n",
    "        mask: torch.Tensor=None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        d_k = query.shape[-1]\n",
    "        \n",
    "        # (batch, head, seq_len, d_k) @ (batch, head, d_k, seq_len) --> (batch, head, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)        \n",
    "\n",
    "        # The mask passed has two components:\n",
    "        # 1. A lookback mask that makes sure the output at a certain position can only depend on the tokens on from previous positions. (USED ONLY ON THE DECODER)\n",
    "        # 2. An ignore mask so that attention score for the padding token [PAD] is zero. (USED BOTH ON THE DECODER AND THE ENCODER)\n",
    "        # If a mask is passed then some of the attention scores are set to zero based on the mask.\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0, -1e09)\n",
    "            \n",
    "        # (batch, head, seq_len, seq_len) which applies softmax to the last dimension\n",
    "        # so that the sum of the probabilities along this dimension equals 1\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        \n",
    "        # (batch, head, seq_len, seq_len) @ (batch, head, seq_len, d_k) --> (batch, head, seq_len, d_k)\n",
    "        return (attention_scores @ value), attention_scores\n",
    "    \n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        # (batch, seq_len, d_model) @ (d_model, d_model) --> (batch, seq_len, d_model)\n",
    "        query: torch.Tensor = self.W_q(q) \n",
    "\n",
    "        # (batch, seq_len, d_model) @ (d_model, d_model) --> (batch, seq_len, d_model)\n",
    "        key: torch.Tensor = self.W_k(k)   \n",
    "        \n",
    "        # (batch, seq_len, d_model) @ (d_model, d_model) --> (batch, seq_len, d_model)\n",
    "        value: torch.Tensor = self.W_v(v) \n",
    "        \n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, head, d_k) --> (batch, head, seq_len, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], HEADS, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], HEADS, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], HEADS, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Here has shape x = (batch, head, seq_len, d_k)\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, self.dropout, mask)\n",
    "        \n",
    "        # (batch, head, seq_len, d_k) --> (batch, seq_len, head, d_k)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # (batch, seq_len, head, d_k) --> (batch, seq_len, d_model)\n",
    "        x = x.contiguous().view(x.shape[0], -1, HEADS * self.d_k)\n",
    "        \n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
    "        return self.W_o(x)\n",
    "    \n",
    "    \n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.norm = nn.LayerNorm(D_MODEL, device=DEVICE)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, sublayer: nn.Module) -> torch.Tensor:\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        \n",
    "            \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection() for _ in range(2)])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, mask))\n",
    "        # x = self.residual_connections[1](x, lambda x: self.self_attention_block(x, x, x, mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x\n",
    "       \n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, decoder_blocks: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.decoder_blocks = decoder_blocks\n",
    "        self.norm = nn.LayerNorm(D_MODEL, device=DEVICE)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.decoder_blocks:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "    \n",
    "    \n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(D_MODEL, vocab_size)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "    \n",
    "    \n",
    "class GPTmodel(nn.Module):\n",
    "    def __init__(self, decoder: Decoder, embed: WordEmbedding, pos_encoder: PositionEncoder, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.decoder = decoder\n",
    "        self.embed = embed\n",
    "        self.pos_encoder = pos_encoder\n",
    "        self.projection_layer = projection_layer\n",
    "    \n",
    "    def decode(self, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        target = self.embed(target)\n",
    "        target = self.pos_encoder(target)\n",
    "        return self.decoder(target, mask)\n",
    "    \n",
    "    def project(self, x: torch.Tensor):\n",
    "        return self.projection_layer(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def build(\n",
    "        vocab_size: int,\n",
    "        state: dict = None\n",
    "    ):\n",
    "        embed = WordEmbedding(vocab_size)\n",
    "        pos_encoder = PositionEncoder()\n",
    "            \n",
    "        # Create N_BLOCKS number of decoders\n",
    "        decoder_blocks = []\n",
    "        for _ in range(N_BLOCKS):\n",
    "            self_attention_block = MultiHeadAttentionBlock()\n",
    "            feed_forward_block = FeedForwardBlock()\n",
    "            \n",
    "            decoder_blocks.append(\n",
    "                DecoderBlock(self_attention_block, feed_forward_block)\n",
    "            )\n",
    "            \n",
    "        decoder = Decoder(nn.ModuleList(decoder_blocks))        \n",
    "        projection_layer = ProjectionLayer(vocab_size)\n",
    "        \n",
    "        transformer = GPTmodel(decoder, embed, pos_encoder, projection_layer)\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        for p in transformer.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "        if state:\n",
    "            transformer.load_state_dict(state)\n",
    "\n",
    "        return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptInferenceEngine:\n",
    "    \n",
    "    def __init__(self, model: GPTmodel, tokenizer: Tokenizer, top_k: int= 5, nucleus_threshold=10) -> None:\n",
    "        self.model = model\n",
    "        self.top_k = top_k\n",
    "        self.tokenizer = tokenizer\n",
    "        self.nucleus_threshold = nucleus_threshold\n",
    "\n",
    "        self.sos_id = self.tokenizer.token_to_id(\"[SOS]\")\n",
    "        self.pad_id = self.tokenizer.token_to_id(\"[PAD]\")\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def size(self, tensor: torch.Tensor) -> int:\n",
    "        return (tensor == self.pad_id).nonzero()[0][1].item() - 1\n",
    "       \n",
    "    @torch.no_grad() \n",
    "    def complete(self, text: str, max_len: int) -> str:\n",
    "        dataset = TextDataset(\n",
    "            dataset=[text],\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        batch_iterator = iter(dataset.batch_iterator(1))\n",
    "        batch = next(batch_iterator)\n",
    "        \n",
    "        # (1, 1, seq_len, seq_len) \n",
    "        decoder_mask: torch.Tensor = batch[\"decoder_mask\"].to(DEVICE)\n",
    "\n",
    "        # (1, seq_len)\n",
    "        decoder_input: torch.Tensor = batch[\"decoder_input\"].to(DEVICE)\n",
    "\n",
    "        tokens = self.size(decoder_input)\n",
    "\n",
    "        # Initialize the decoder input with the continue token\n",
    "        predicted_token = None\n",
    "        while tokens < max_len:\n",
    "            # (1, seq_len, d_model)\n",
    "            decoder_out = self.model.decode(decoder_input, decoder_mask)\n",
    "\n",
    "            # (1, d_model)\n",
    "            temp = decoder_out[:, tokens + 1]\n",
    "            \n",
    "            # (1, d_model) --> (1, vocab_size)\n",
    "            logits = self.model.project(temp)\n",
    "            \n",
    "            # Evaluate the probability distribution across the vocab_size \n",
    "            # dimension using softmax\n",
    "            # (1, vocab_size)\n",
    "            probab_distribution = torch.softmax(logits, dim=1)\n",
    "            \n",
    "            # Greedily pick the token with the highest probability\n",
    "            _, predicted_token = torch.max(probab_distribution, dim=1)\n",
    "            \n",
    "            # Add the predicted token to the decoder input for the subsequent iterations\n",
    "            decoder_input[0, tokens + 1] = predicted_token.item()\n",
    "\n",
    "            tokens += 1\n",
    "\n",
    "        # Remove the batch dimension\n",
    "        # (1, seq_len) ---> (seq_len,)\n",
    "        decoder_input = decoder_input.squeeze(0)\n",
    "\n",
    "        return self.tokenizer.decode(decoder_input.detach().cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard\n",
    "%tensorboard --logdir \"/content/drive/My Drive/Text-Generation/logs/gpt_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer() -> Tokenizer:\n",
    "    tokenizer: Tokenizer = Tokenizer.from_file(TOKENIZER_FILEPATH)\n",
    "    tokenizer.enable_truncation(max_length=SEQ_LEN)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def get_dataset() -> tuple[TextDataset, TextDataset, TextDataset]:\n",
    "    with open(DATASET_PATH, 'r', encoding='utf-8') as file:\n",
    "        texts = file.readlines()\n",
    "    \n",
    "    train_size = int(0.8 * len(texts))\n",
    "    test_size = int(0.15 * len(texts))\n",
    "    val_size = len(texts) - train_size - test_size\n",
    "    \n",
    "    train_test_raw, val_raw = random_split(texts, (train_size+test_size, val_size))\n",
    "    train_raw, test_raw = random_split(train_test_raw, (train_size, test_size))\n",
    "    \n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    train_dataset = TextDataset(train_raw, tokenizer)\n",
    "    val_dataset = TextDataset(val_raw, tokenizer)\n",
    "    test_dataset = TextDataset(test_raw, tokenizer)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    \n",
    "@torch.no_grad()\n",
    "def validate(model: GPTmodel, val_dataset: TextDataset, loss_func: nn.CrossEntropyLoss):\n",
    "    model.eval()\n",
    "\n",
    "    batch_iterator = val_dataset.batch_iterator(BATCH_SIZE)\n",
    "\n",
    "    val_loss = 0\n",
    "    for batch in batch_iterator:\n",
    "        # Retrieve the data points from the current batch\n",
    "        # (batches, seq_len)\n",
    "        decoder_input = batch[\"decoder_input\"].to(DEVICE)\n",
    "\n",
    "        # (batches, 1, seq_len, seq_len)\n",
    "        decoder_mask = batch[\"decoder_mask\"].to(DEVICE)\n",
    "\n",
    "        # (batches, seq_len, d_model)\n",
    "        label: torch.Tensor = batch['label'].to(DEVICE)\n",
    "\n",
    "\n",
    "        # (batches, seq_len, d_model)\n",
    "        decoder_output = model.decode(decoder_input, decoder_mask)\n",
    "\n",
    "        # (batches, seq_len, vocab_size)\n",
    "        proj_output: torch.Tensor = model.project(decoder_output)\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        loss: torch.Tensor = loss_func(\n",
    "            # (batches, seq_len, vocab_size) --> (batches*seq_len, vocab_size)\n",
    "            proj_output.view(-1, val_dataset.tokenizer.get_vocab_size()),\n",
    "\n",
    "            # (batches, seq_len) --> (batches * seq_len, )\n",
    "            label.view(-1)\n",
    "        )\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        break\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model: GPTmodel, test_dataset: TextDataset):\n",
    "    print(f\"Testing started on `{DEVICE}` device\")\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss(ignore_index=test_dataset.tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(DEVICE)\n",
    "\n",
    "    batch_iterator = tqdm(test_dataset.batch_iterator(BATCH_SIZE), desc=f\"Evaluating model on test dataset\", colour=\"GREEN\")\n",
    "\n",
    "    evaluation_loss = 0\n",
    "    # Iterate through the batches\n",
    "    for batch in batch_iterator:\n",
    "        # (batches, seq_len)\n",
    "        decoder_input = batch[\"decoder_input\"].to(DEVICE)\n",
    "\n",
    "        # (bathes, 1, seq_len, seq_len)\n",
    "        decoder_mask = batch[\"decoder_mask\"].to(DEVICE)\n",
    "\n",
    "        # (batches, seq_len)\n",
    "        label: torch.Tensor = batch['label'].to(DEVICE)\n",
    "\n",
    "        # (batches, seq_len, d_model)\n",
    "        decoder_output = model.decode(decoder_input, decoder_mask)\n",
    "\n",
    "        # (batches, seq_len, tgt_vocab_size)\n",
    "        logits: torch.Tensor = model.project(decoder_output)\n",
    "\n",
    "        # Compute the training loss\n",
    "        test_loss: torch.Tensor = loss_func(\n",
    "            # (batches, seq_len, tgt_vocab_size)  -->  (batches*seq_len, tgt_vocab_size)\n",
    "            logits.view(-1, test_dataset.tokenizer.get_vocab_size()),\n",
    "\n",
    "            # (batches, seq_len)   -->   (batches * seq_len, )\n",
    "            label.view(-1)\n",
    "        )\n",
    "\n",
    "        # Add the calculated test loss as a postfix to the progress bar shown by tqdm\n",
    "        batch_iterator.set_postfix({\"test_loss\": f\"{test_loss.item():6.3f}\"})\n",
    "\n",
    "        evaluation_loss += test_loss.item()\n",
    "\n",
    "    avg_loss = evaluation_loss / len(batch_iterator)\n",
    "    print(f\"\\nTesting finished with an average cross-entropy loss of {avg_loss}\")\n",
    "\n",
    "    \n",
    "def train(model: GPTmodel, train_dataset: TextDataset, val_dataset: TextDataset) -> None:   \n",
    "    # Configure Tensorboard\n",
    "    writer = SummaryWriter(TB_LOG_DIR)\n",
    "    \n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=INIT_LR, eps=1e-09)\n",
    "    \n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    if PRELOAD_MODEL_FILEPATH:\n",
    "        model_filename = f\"{MODELS_FOLDER}/{PRELOAD_MODEL_FILEPATH}.pt\"\n",
    "        print(f\"Preloading model {model_filename}\")\n",
    "\n",
    "        state = torch.load(model_filename)\n",
    "        initial_epoch = state[\"epoch\"] + 1\n",
    "        global_step = state[\"global_step\"]\n",
    "        training_loss = state[\"training_loss\"]\n",
    "        validation_loss = state[\"validation_loss\"]\n",
    "\n",
    "        model.load_state_dict(state[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss(ignore_index=train_dataset.tokenizer.token_to_id('[PAD]'), label_smoothing=0.1).to(DEVICE)\n",
    "\n",
    "    batch_iterator = train_dataset.batch_iterator(BATCH_SIZE)\n",
    "\n",
    "    for epoch in range(initial_epoch, EPOCHS):\n",
    "        batch_iterator = tqdm(batch_iterator, desc=f\"Processing epoch {epoch: 02d}\", colour=\"BLUE\")\n",
    "        \n",
    "        for batch in batch_iterator:\n",
    "            model.train() \n",
    "                 \n",
    "            # (batch, seq_len)\n",
    "            decoder_input = batch[\"decoder_input\"].to(DEVICE)\n",
    "\n",
    "            # (batch, 1, seq_len, seq_len)\n",
    "            decoder_mask = batch[\"decoder_mask\"].to(DEVICE)\n",
    "            \n",
    "            # (batch, seq_len)\n",
    "            label: torch.Tensor = batch['label'].to(DEVICE)\n",
    "            \n",
    "            # (batch, seq_len, d_model)\n",
    "            decoder_output = model.decode(decoder_input, decoder_mask)\n",
    "\n",
    "            # (batch, seq_len, vocab_size)\n",
    "            logits: torch.Tensor = model.project(decoder_output)\n",
    "\n",
    "                        \n",
    "            # Compute the cross-entropy loss\n",
    "            batch_loss = loss_func.forward(\n",
    "                # (batch, seq_len, vocab_size) --> (batch*seq_len, vocab_size)\n",
    "                logits.view(-1, train_dataset.tokenizer.get_vocab_size()),\n",
    "\n",
    "                # (batch, seq_len) --> (batch * seq_len, )\n",
    "                label.view(-1)\n",
    "            )\n",
    "            training_loss += batch_loss.item()\n",
    "\n",
    "            if global_step % 200 == 0:\n",
    "                # Evaluate the model on the validation dataset(aka unseen data)\n",
    "                validation_loss += validate(model, val_dataset, loss_func)\n",
    "                \n",
    "                # Log the training and validation loss on tensorboard\n",
    "                writer.add_scalars(\"Cross-Entropy-Loss\", { \"Training\": training_loss / (global_step + 1), \"Validation\": validation_loss / ((global_step + 1) // 200 + 1) }, global_step)\n",
    "            else:\n",
    "                writer.add_scalars(\"Cross-Entropy-Loss\", { \"Training\": training_loss / (global_step + 1) }, global_step)\n",
    "                \n",
    "            writer.flush()\n",
    "            \n",
    "            batch_iterator.set_postfix({\"train_loss\": f\"{training_loss / (global_step + 1):6.3f}\", \"val_loss\": f\"{validation_loss / ((global_step + 1) // 200 + 1):6.3f}\"})\n",
    "\n",
    "            # Perform the backward pass on the computation graph built during the forward pass, \n",
    "            # in order to calculate the grad for each of the intermediate and leaf tensors on the computation graph\n",
    "            batch_loss.backward()\n",
    "            \n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Zero the gradients of the model parameters to prevent gradient accumulation \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1\n",
    "        \n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = f\"{MODELS_FOLDER}/gpt_model-avgTrainLoss-{training_loss / global_step:6.3f}_avgValLoss-{validation_loss / (global_step // 200 + 1):6.3f}.pt\"\n",
    "        \n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"global_step\": global_step,\n",
    "            \"training_loss\": training_loss,\n",
    "            \"validation_loss\": validation_loss,\n",
    "            \"model_hyperparams\":{\n",
    "                \"D_MODEL\": D_MODEL,\n",
    "                \"N_BLOCKS\": N_BLOCKS,\n",
    "                \"HEADS\": HEADS,\n",
    "                \"DROPOUT\": DROPOUT,\n",
    "                \"DFF\": DFF,\n",
    "                \"BATCH_SIZE\": BATCH_SIZE,\n",
    "                \"INIT_LR\": INIT_LR\n",
    "            }\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training started on `{DEVICE}` device\")\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "\n",
    "model = GPTmodel.build(train_dataset.tokenizer.get_vocab_size()).to(DEVICE) \n",
    "\n",
    "train(model, train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Total Parameters: {total_params}\")\n",
    "print(f\"Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "print(f\"Model Size(MB): {total_params * 4 / (1024 ** 2):.2f}MB\")\n",
    "\n",
    "tokenizer: Tokenizer = get_tokenizer()\n",
    "inference_engine = GptInferenceEngine(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Write an incomplete short amharic text: \")\n",
    "predicted = inference_engine.complete(user_input, SEQ_LEN)\n",
    "print(f\"\\n Predicted: {predicted}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
